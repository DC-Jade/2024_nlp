{"cells":[{"cell_type":"markdown","metadata":{"id":"eS1h8tqidd9I"},"source":["**Tasks**\n","\n","In Task 2, we successfully implemented N-gram models, which laid the groundwork for understanding how words and their sequences can be analyzed to predict and interpret language patterns. Building upon that foundational knowledge, this assignment focuses specifically on the application of a bigram (2-gram) model to calculate the probability of a given sentence.\n","\n","1.   Implement Bigram Model Construction: Calculate and store the probability of each bigram based on the provided corpus.\n","2.   Calculate the Probability of a Given Sentence: Use the constructed bigram model to calculate the probability of a specific sentence.\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"I_MwuhaddMap"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'this': 1, 'is': 2, 'an': 2, 'example': 2, 'sentence': 1, 'for': 1, 'the': 1, 'corpus': 1, 'it': 1, 'just': 1}\n","{'this is': 1, 'is an': 1, 'an example': 2, 'example sentence': 1, 'sentence for': 1, 'for the': 1, 'the corpus': 1, 'corpus it': 1, 'it is': 1, 'is just': 1, 'just an': 1}\n","{'this is': 1.0, 'is an': 0.5, 'an example': 1.0, 'example sentence': 0.5, 'sentence for': 1.0, 'for the': 1.0, 'the corpus': 1.0, 'corpus it': 1.0, 'it is': 1.0, 'is just': 0.5, 'just an': 1.0}\n","{'this': 1, 'is': 1, 'an': 1, 'example': 1}\n","{'this': 1, 'is': 1, 'an': 1, 'example': 1}\n","{'this': 1, 'is': 1, 'an': 1, 'example': 1}\n","Sentence Probability: 0.5\n"]}],"source":["# from importlib import import_module\n","# %run  \"2.2 Building N-gram Language Models.py\" import *\n","\n","# function in 2.2\n","def process_text(text):\n","    text = text.lower()\n","    return text\n","\n","def tokenize(text):\n","    # Replace punctuation marks with space + punctuation\n","    punctuation_pattern = r'([.,!?;:])'\n","    text = re.sub(punctuation_pattern, r' \\1', text)\n","    print(text)\n","    return text.split()\n","\n","def padSequence(sequence_, ngram_, is_pad_left_ = True, is_pad_right_ = True,\n","                pad_left_token_ = \"<s>\", pad_right_token_ = \"</s>\"):\n","    assert isinstance(sequence_, list), \"sequence_ is not list\"\n","    if (is_pad_left_):\n","        i = 0\n","        while (i < ngram_ - 1):\n","            sequence_.insert(0, pad_left_token_)\n","            i += 1\n","    if (is_pad_right_):\n","        i = 0\n","        while (i < ngram_ - 1):\n","            sequence_.append(pad_right_token_)\n","            i += 1\n","    return sequence_\n","\n","def generate_ngrams(words, n):\n","    # TODO: Implement the logic to generate n-grams from the list of words\n","    # words = padSequence(sequence_=words, ngram_=n)\n","    res = []\n","    res = [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n","    # print(res)\n","    return res\n","\n","def count_ngrams(ngrams):\n","    ngram_counts = {}\n","    for ngram in ngrams:\n","        if ngram in ngram_counts:\n","            ngram_counts[ngram] += 1\n","        else:\n","            ngram_counts[ngram] = 1\n","    return ngram_counts\n","\n","def is_near(lhs, rhs, delta=0.01):\n","    return(abs(lhs - rhs) < delta)\n","\n","def calculate_ngram_probabilities(ngram_counts):\n","    total_ngrams = sum(ngram_counts.values())\n","    ngram_probabilities = {ngram: count / total_ngrams for ngram, count in ngram_counts.items()}\n","    # print(sum(ngram_probabilities.values()))\n","    assert is_near(sum(ngram_probabilities.values()),1), \"error: sum(prob) != 1\"\n","    return ngram_probabilities\n","\n","def query_ngram_probability(ngram, ngram_probabilities):\n","    # print(ngram_probabilities)\n","    # print(ngram_probabilities.get(ngram,1))\n","    return ngram_probabilities.get(ngram, 0)\n","\n","\n","\n","# my_functions = import_module(\"2-1-maximum-likelihood-estimation-mle-implementation.ipynb\")\n","def tokenize(text):\n","    return text.lower().split()\n","alpha = 1\n","def build_bigram_model(corpus):\n","    \"\"\"\n","    TODO: Build the bigram model\n","    - Count the occurrences of each bigram\n","    - Calculate the probability of each bigram based on counts\n","    \"\"\"\n","    bigram_counts = {}\n","    unigram_counts = {}\n","    # Your code here to populate bigram_counts and unigram_counts\n","    words = tokenize(corpus)\n","    n_words = len(words)\n","    unigrams = generate_ngrams(words, 1)\n","    unigram_counts = count_ngrams(unigrams)\n","    bigrams = generate_ngrams(words, 2)\n","    bigram_counts = count_ngrams(bigrams)\n","\n","    print(unigram_counts)\n","    print(bigram_counts)\n","\n","    # Calculate bigram probabilities\n","    bigram_probabilities = {}\n","    # Your code here to calculate probabilities from counts\n","    for bigram, count in bigram_counts.items():\n","        prev_word = bigram.split(sep = \" \")[0]\n","        succ_word = bigram.split(sep = \" \")[1]\n","        conditional_prob = count / unigram_counts[prev_word]\n","        bigram_probabilities.update({bigram: conditional_prob})\n","    print(bigram_probabilities)\n","    return bigram_probabilities\n","\n","\n","\n","def calculate_sentence_probability(sentence, bigram_probabilities):\n","    \"\"\"\n","    TODO: Calculate the probability of a sentence using the bigram model\n","    - Split the sentence into words\n","    - Calculate the probability of each bigram in the sentence and multiply them to get the sentence probability\n","    \"\"\"\n","    words = tokenize(sentence)\n","    probability = 1\n","    # Your code here to calculate the sentence probability\n","    n_words = len(words)\n","    bigrams = bigram_probabilities.keys()\n","    unigrams = generate_ngrams(words, 1)\n","    unigram_counts = count_ngrams(unigrams)\n","    # for bigram in bigrams:\n","    for i in range(n_words - 1):\n","        prev_word = words[i]\n","        succ_word = words[i + 1]\n","        bigram = prev_word + \" \" + succ_word\n","        print(unigram_counts)\n","        cur_unigram_count = unigram_counts[prev_word]\n","        probability *= bigram_probabilities.get(bigram, alpha / (cur_unigram_count + cur_unigram_count * alpha))\n","        # print(bigram_probabilities[bigram])\n","\n","    return probability\n","\n","# Example corpus\n","corpus = \"this is an example sentence for the corpus it is just an example\"\n","# Build the bigram model\n","bigram_probabilities = build_bigram_model(corpus)\n","\n","# Calculate the probability of a given sentence\n","sentence = \"this is an example\"\n","# sentece = corpus\n","# sentence = \"oov is\"\n","probability = calculate_sentence_probability(sentence, bigram_probabilities)\n","print(f\"Sentence Probability: {probability}\")\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1bH1vqaupc1r7uM_0Hd9FaBBM499ccMao","timestamp":1710553808038}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
