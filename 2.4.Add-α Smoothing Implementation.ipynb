{"cells":[{"cell_type":"markdown","metadata":{"id":"Rm9lG-tNbN89"},"source":["\n","**Tasks**\n","1.   Implement the Add-α Smoothing Technique: Complete the add_alpha_smoothing function to calculate smoothed probabilities for each word in the unigram model, considering the smoothing parameter α and the total size of the vocabulary, including OOV words.\n","2.   Test the Effect of Smoothing: Use the smoothed model to calculate the probability of a set of test sentences, paying special attention to how OOV words are handled.\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ttKthwmnbKkJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'this': 1, 'is': 1, 'a': 1, 'sample': 2, 'corpus': 1, 'with': 1, 'sentences': 1}\n","1.0\n","Sentence: 'this is a test' Probability: 6.724597831690787e-05\n","Sentence: 'oov words here' Probability: 2.7826474107465846e-05\n"]}],"source":["def tokenize(text):\n","    return text.lower().split()\n","\n","def build_unigram_model(text):\n","    words = tokenize(text)\n","    unigram_counts = {}\n","    for word in words:\n","        if word in unigram_counts:\n","            unigram_counts[word] += 1\n","        else:\n","            unigram_counts[word] = 1\n","    return unigram_counts\n","\n","def is_near(lhs, rhs, delta=0.01):\n","    return(abs(lhs - rhs) < delta)\n","\n","def add_alpha_smoothing(unigram_counts, alpha, vocabulary_size):\n","    \"\"\"\n","    Apply Add-α smoothing to the unigram model.\n","    TODO: Implement the calculation of smoothed probabilities.\n","    \"\"\"\n","    smoothed_probabilities = {}\n","    # Your code here to calculate smoothed probabilities for each word\n","    # total_counts = sum(unigram_counts.values())\n","    n_token = len(unigram_counts.keys())\n","    # vocabulary_size_add_alpha = vocabulary_size + n_token * alpha\n","    vocabulary_size_add_alpha = sum(unigram_counts.values()) + n_token * alpha\n","    # vocabulary_size_add_alpha = sum(unigram_counts.values()) + vocabulary_size * alpha\n","    for unigram, count in unigram_counts.items():\n","        smoothed_prob = (count + alpha) / vocabulary_size_add_alpha\n","        smoothed_probabilities.update({unigram: smoothed_prob})\n","    print(sum(smoothed_probabilities.values()))\n","    \n","    assert is_near(sum(smoothed_probabilities.values()),1), \"error: sum(prob) != 1\"\n","    return smoothed_probabilities\n","\n","# Example usage\n","corpus = \"this is a sample corpus with sample sentences\"\n","unigram_counts = build_unigram_model(corpus)\n","print(unigram_counts)\n","\n","# Assume the vocabulary size is known or estimated\n","vocabulary_size = len(unigram_counts) + 10  # Adding hypothetical OOV words\n","\n","# TODO: Apply Add-α smoothing with a chosen alpha value\n","alpha = 0.5\n","smoothed_probabilities = add_alpha_smoothing(unigram_counts, alpha, vocabulary_size)\n","\n","# TODO: Test with sentences including OOV words\n","test_sentences = [\"this is a test\", \"oov words here\"]\n","# Implement the testing logic to observe the effect of Add-α smoothing\n","\n","for sentence in test_sentences:\n","    words = tokenize(sentence)\n","    sentence_probability = 1\n","    for word in words:\n","        # Handle OOV words by assigning them a default probability\n","        # print((word, word_probability))\n","        word_probability = smoothed_probabilities.get(word, alpha / (sum(unigram_counts.values()) + alpha * vocabulary_size))\n","        sentence_probability *= word_probability\n","    print(f\"Sentence: '{sentence}' Probability: {sentence_probability}\")\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1mo-qk7_yCDDGbgWv7l87ccftk-eHdsCY","timestamp":1710554396866}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
