{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1mo-qk7_yCDDGbgWv7l87ccftk-eHdsCY","timestamp":1710554396866}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","**Tasks**\n","1.   Implement the Add-α Smoothing Technique: Complete the add_alpha_smoothing function to calculate smoothed probabilities for each word in the unigram model, considering the smoothing parameter α and the total size of the vocabulary, including OOV words.\n","2.   Test the Effect of Smoothing: Use the smoothed model to calculate the probability of a set of test sentences, paying special attention to how OOV words are handled.\n","\n"],"metadata":{"id":"Rm9lG-tNbN89"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttKthwmnbKkJ"},"outputs":[],"source":["def tokenize(text):\n","    return text.lower().split()\n","\n","def build_unigram_model(text):\n","    words = tokenize(text)\n","    unigram_counts = {}\n","    for word in words:\n","        if word in unigram_counts:\n","            unigram_counts[word] += 1\n","        else:\n","            unigram_counts[word] = 1\n","    return unigram_counts\n","\n","def add_alpha_smoothing(unigram_counts, alpha, vocabulary_size):\n","    \"\"\"\n","    Apply Add-α smoothing to the unigram model.\n","    TODO: Implement the calculation of smoothed probabilities.\n","    \"\"\"\n","    smoothed_probabilities = {}\n","    # Your code here to calculate smoothed probabilities for each word\n","    return smoothed_probabilities\n","\n","# Example usage\n","corpus = \"this is a sample corpus with sample sentences\"\n","unigram_counts = build_unigram_model(corpus)\n","\n","# Assume the vocabulary size is known or estimated\n","vocabulary_size = len(unigram_counts) + 10  # Adding hypothetical OOV words\n","\n","# TODO: Apply Add-α smoothing with a chosen alpha value\n","alpha = 0.5\n","smoothed_probabilities = add_alpha_smoothing(unigram_counts, alpha, vocabulary_size)\n","\n","# TODO: Test with sentences including OOV words\n","test_sentences = [\"this is a test\", \"oov words here\"]\n","# Implement the testing logic to observe the effect of Add-α smoothing\n","\n","for sentence in test_sentences:\n","    words = tokenize(sentence)\n","    sentence_probability = 1\n","    for word in words:\n","        # Handle OOV words by assigning them a default probability\n","        word_probability = smoothed_probabilities.get(word, alpha / (sum(unigram_counts.values()) + alpha * vocabulary_size))\n","        sentence_probability *= word_probability\n","    print(f\"Sentence: '{sentence}' Probability: {sentence_probability}\")\n","\n"]}]}