{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample word_counts dictionary (normally, you would compute this from a text corpus)\n",
    "word_counts = {'the': 100, 'of': 50, 'and': 25, 'to': 75, 'a': 50}\n",
    "\n",
    "def calculate_total_words(word_counts):\n",
    "    # TODO: Calculate the total number of words in the corpus\n",
    "    # for count_ in word_counts.values():\n",
    "    #     total_words += count_\n",
    "    # return(total_words)\n",
    "    res = sum(word_counts.values())\n",
    "    return res \n",
    "\n",
    "def calculate_probabilities(word_counts, total_words):\n",
    "    probabilities = {}\n",
    "    # TODO: Implement the MLE calculation here\n",
    "    # Hint: Loop through the word_counts dictionary and for each word,\n",
    "    # calculate its probability as count / total_words, then add it to the probabilities dictionary\n",
    "    for word, count in word_counts.items():\n",
    "        prob = 0\n",
    "        prob = count / total_words\n",
    "        probabilities.update({word:prob})\n",
    "    return probabilities\n",
    "\n",
    "def display_probabilities(probabilities):\n",
    "    # Sort and display the probabilities in descending order\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, prob in sorted_probabilities:\n",
    "        print(f\"{word}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 0.3333\n",
      "to: 0.2500\n",
      "of: 0.1667\n",
      "a: 0.1667\n",
      "and: 0.0833\n",
      "300\n",
      "True\n",
      "{'the': 0.3333333333333333, 'of': 0.16666666666666666, 'and': 0.08333333333333333, 'to': 0.25, 'a': 0.16666666666666666}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "total_counts_t = 300\n",
    "probs_t = {'the': 100/300, 'of': 50/300, 'and': 25/300, 'to': 75/300, 'a': 50/300}\n",
    "total_counts = calculate_total_words(word_counts=word_counts)\n",
    "probs = calculate_probabilities(word_counts=word_counts, total_words=total_counts)\n",
    "display_probabilities(probs)\n",
    "print(total_counts)\n",
    "print(total_counts == total_counts)\n",
    "print(probs)\n",
    "print(probs == probs_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequence(sequence_, ngram_ = 2, is_pad_left_ = True, is_pad_right_ = True,\n",
    "                pad_left_token_ = \"<s>\", pad_right_token_ = \"</s>\"):\n",
    "    # sequence_ = iter(sequence_) \n",
    "    # if (isinstance(type(sequence_), list)):\n",
    "    assert isinstance(sequence_, list), \"sequence_ is not list\"\n",
    "    \n",
    "    if (is_pad_left_):\n",
    "        i = 0\n",
    "        # sequence_ = chain((pad_left_token_, ) * (ngram_ - 1), sequence_)\n",
    "        while (i < ngram_ - 1):\n",
    "            sequence_.insert(0, pad_left_token_)\n",
    "            i += 1\n",
    "    if (is_pad_right_):\n",
    "        i = 0\n",
    "        while (i < ngram_ - 1):\n",
    "            # sequence_ = chain(sequence_, (pad_right_token_, ) * (ngram_ - 1))\n",
    "            sequence_.append(pad_right_token_)\n",
    "            i += 1\n",
    "    return sequence_\n",
    "\n",
    "def generate_ngrams(words, n):\n",
    "    # TODO: Implement the logic to generate n-grams from the list of words\n",
    "    words = padSequence(sequence_=words, ngram_=n)\n",
    "    words_list = [words[i:i + n] for i in range(len(words) - n + 1)]\n",
    "    res = dict(words_list)\n",
    "    return res\n",
    "\n",
    "# def ngrams(tokens): \n",
    "#     sequence = pad_sequence(sequence, n, **kwargs)\n",
    "\n",
    "#     # Creates the sliding window, of n no. of items.\n",
    "#     # `iterables` is a tuple of iterables where each iterable is a window of n items.\n",
    "#     iterables = tee(sequence, n)\n",
    "\n",
    "#     for i, sub_iterable in enumerate(iterables):  # For each window,\n",
    "#         for _ in range(i):  # iterate through every order of ngrams\n",
    "#             next(sub_iterable, None)  # generate the ngrams within the window.\n",
    "#     return zip(*iterables)  # Unpack and flattens the iterables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 'the',\n",
       " 'the': 'of',\n",
       " 'of': 'and',\n",
       " 'and': 'to',\n",
       " 'to': 'a',\n",
       " 'a': '</s>',\n",
       " '</s>': '</s>'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(sequence_)\n",
    "sequence_ = padSequence(list(word_counts.keys()))\n",
    "generate_ngrams(sequence_, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import re\n",
    "\n",
    "documents = [\n",
    "    (\"This movie was an excellent adventure with fantastic visuals\", \"Positive\"),\n",
    "    (\"I loved the storyline and the characters were very compelling\", \"Positive\"),\n",
    "    (\"One of the best movies I have seen in recent years\", \"Positive\"),\n",
    "    (\"A thrilling movie that kept me on the edge of my seat from start to finish\", \"Positive\"),\n",
    "    (\"The cinematography is stunning and the plot is intriguing\", \"Positive\"),\n",
    "    (\"This movie was a waste of time and the plot was very predictable\", \"Negative\"),\n",
    "    (\"I was disappointed with the lead actor's performance and the movie was dull\", \"Negative\"),\n",
    "    (\"Not worth the ticket price - I found it to be quite boring\", \"Negative\"),\n",
    "    (\"The movie failed to deliver on its promise of a thrilling experience\", \"Negative\"),\n",
    "    (\"It was an underwhelming movie that left much to be desired\", \"Negative\"),\n",
    "    (\"The script felt rushed and the ending was unsatisfying\", \"Negative\"),\n",
    "    (\"An absolute masterpiece, with brilliant direction and performances\", \"Positive\"),\n",
    "    (\"The film’s pacing was perfect, and the suspense was maintained throughout\", \"Positive\"),\n",
    "    (\"A highly recommended movie that appeals to both critics and the general audience\", \"Positive\"),\n",
    "    (\"A unique and beautiful take on an otherwise familiar story\", \"Positive\"),\n",
    "    (\"The dialogue was uninspired, and the pacing was slow\", \"Negative\"),\n",
    "    (\"Poor character development and a predictable plot\", \"Negative\"),\n",
    "    (\"I found the film to be overrated and lacking in originality\", \"Negative\"),\n",
    "    (\"The special effects were mediocre at best, failing to impress\", \"Negative\"),\n",
    "    (\"A forgettable film that doesn't leave a lasting impression\", \"Negative\")\n",
    "]\n",
    "\n",
    "# Separate the documents and their labels\n",
    "docs, labels = zip(*documents)\n",
    "\n",
    "# Preprocess and split the dataset\n",
    "def preprocess_documents(docs):\n",
    "    preprocessed_docs = []\n",
    "    for doc in docs:\n",
    "        # Convert to lowercase\n",
    "        doc = doc.lower()\n",
    "        # Remove punctuation\n",
    "        doc = re.sub(r'\\W', ' ', doc)\n",
    "        # Tokenize (in this context, simply splitting by whitespace)\n",
    "        tokens = doc.split()\n",
    "        # Rejoin tokens into a single string\n",
    "        preprocessed_doc = ' '.join(tokens)\n",
    "        preprocessed_docs.append(preprocessed_doc)\n",
    "    return preprocessed_docs\n",
    "\n",
    "# Preprocessed documents\n",
    "preprocessed_docs = preprocess_documents(docs)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "docs_train, docs_test, labels_train, labels_test = train_test_split(preprocessed_docs, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "# TODO: Fit and transform the training data with CountVectorizer\n",
    "X_train = vectorizer.fit_transform(docs_train)\n",
    "# TODO: Transform the testing data with the fitted CountVectorizer\n",
    "X_test = vectorizer.transform(docs_test)\n",
    "\n",
    "# TODO: Train the Naïve Bayes Classifier using MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, labels_train)\n",
    "\n",
    "# TODO: Test the classifier with the test set and calculate the accuracy\n",
    "def test_classifier(model, X_test, labels_test):\n",
    "    # Implement the prediction and calculation of accuracy\n",
    "    predict_labels = model.predict(X_test)\n",
    "    res = sum(predict_labels == labels_test) / len(labels_test)\n",
    "    # for tup in zip(predict_labels, labels_test):\n",
    "    #     print(tup)\n",
    "    print(res)\n",
    "\n",
    "# Call test_classifier and print the accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Negative', 'Positive')\n",
      "('Negative', 'Negative')\n",
      "('Negative', 'Negative')\n",
      "('Negative', 'Positive')\n",
      "('Positive', 'Negative')\n",
      "['Positive', 'Negative', 'Negative', 'Positive', 'Negative']\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "test_classifier(model=model,   X_test= X_test,labels_test=labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'this is a test' Probability: 1.187129928732633e-05\n",
      "Sentence: 'oov words here' Probability: 2.7826474107465846e-05\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_unigram_model(text):\n",
    "    words = tokenize(text)\n",
    "    unigram_counts = {}\n",
    "    for word in words:\n",
    "        if word in unigram_counts:\n",
    "            unigram_counts[word] += 1\n",
    "        else:\n",
    "            unigram_counts[word] = 1\n",
    "    return unigram_counts\n",
    "\n",
    "def add_alpha_smoothing(unigram_counts, alpha, vocabulary_size):\n",
    "    \"\"\"\n",
    "    Apply Add-α smoothing to the unigram model.\n",
    "    TODO: Implement the calculation of smoothed probabilities.\n",
    "    \"\"\"\n",
    "    smoothed_probabilities = {}\n",
    "    # Your code here to calculate smoothed probabilities for each word\n",
    "    # total_counts = sum(unigram_counts.values())\n",
    "    n_token = len(unigram_counts.keys())\n",
    "    vocabulary_size_add_alpha = vocabulary_size + n_token * alpha\n",
    "    for unigram, count in unigram_counts.items():\n",
    "        smoothed_prob = (count + alpha) / vocabulary_size_add_alpha\n",
    "        # print(unigram, count, alpha, vocabulary_size_add_alpha, smoothed_prob)\n",
    "        smoothed_probabilities.update({unigram: smoothed_prob})\n",
    "    return smoothed_probabilities\n",
    "\n",
    "# Example usage\n",
    "corpus = \"this is a sample corpus with sample sentences\"\n",
    "unigram_counts = build_unigram_model(corpus)\n",
    "\n",
    "# Assume the vocabulary size is known or estimated\n",
    "vocabulary_size = len(unigram_counts) + 10  # Adding hypothetical OOV words\n",
    "\n",
    "# TODO: Apply Add-α smoothing with a chosen alpha value\n",
    "alpha = 0.5\n",
    "smoothed_probabilities = add_alpha_smoothing(unigram_counts, alpha, vocabulary_size)\n",
    "\n",
    "# TODO: Test with sentences including OOV words\n",
    "test_sentences = [\"this is a test\", \"oov words here\"]\n",
    "# Implement the testing logic to observe the effect of Add-α smoothing\n",
    "for sentence in test_sentences:\n",
    "    words = tokenize(sentence)\n",
    "    sentence_probability = 1\n",
    "    for word in words:\n",
    "        # Handle OOV words by assigning them a default probability\n",
    "        word_probability = smoothed_probabilities.get(word, alpha / (sum(unigram_counts.values()) + alpha * vocabulary_size))\n",
    "        # print(word, word_probability)\n",
    "        sentence_probability *= word_probability\n",
    "    print(f\"Sentence: '{sentence}' Probability: {sentence_probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_bigram_model(corpus):\n",
    "    \"\"\"\n",
    "    TODO: Build the bigram model\n",
    "    - Count the occurrences of each bigram\n",
    "    - Calculate the probability of each bigram based on counts\n",
    "    \"\"\"\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "    # Your code here to populate bigram_counts and unigram_counts\n",
    "    words = tokenize(corpus)\n",
    "    pad_left_token = \"<s>\"\n",
    "    right_pad_token = \"</s>\"\n",
    "    words.insert(0, pad_left_token)\n",
    "    words.append(right_pad_token)\n",
    "    ngram = 2\n",
    "    bigram_counts = {words[i:i + ngram] for i in range(len(words) - ngram + 1)}\n",
    "    ngram = 1\n",
    "    unigram_counts = {words[i:i + ngram] for i in range(len(words) - ngram + 1)}\n",
    "\n",
    "    # Calculate bigram probabilities\n",
    "    bigram_probabilities = {}\n",
    "    # Your code here to calculate probabilities from counts\n",
    "    total_bigram_counts = len(bigram_counts.keys())\n",
    "    bigram_probabilities = {bigram: (count / total_bigram_counts) for bigram, count in bigram_counts.items()}\n",
    "    return bigram_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis is an example\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbuild_bigram_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 19\u001b[0m, in \u001b[0;36mbuild_bigram_model\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m     17\u001b[0m words\u001b[38;5;241m.\u001b[39mappend(right_pad_token)\n\u001b[1;32m     18\u001b[0m ngram \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 19\u001b[0m bigram_counts \u001b[38;5;241m=\u001b[39m {words[i:i \u001b[38;5;241m+\u001b[39m ngram] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m-\u001b[39m ngram \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)}\n\u001b[1;32m     20\u001b[0m ngram \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m unigram_counts \u001b[38;5;241m=\u001b[39m {words[i:i \u001b[38;5;241m+\u001b[39m ngram] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m-\u001b[39m ngram \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)}\n",
      "Cell \u001b[0;32mIn[53], line 19\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m words\u001b[38;5;241m.\u001b[39mappend(right_pad_token)\n\u001b[1;32m     18\u001b[0m ngram \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 19\u001b[0m bigram_counts \u001b[38;5;241m=\u001b[39m {words[i:i \u001b[38;5;241m+\u001b[39m ngram] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m-\u001b[39m ngram \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)}\n\u001b[1;32m     20\u001b[0m ngram \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m unigram_counts \u001b[38;5;241m=\u001b[39m {words[i:i \u001b[38;5;241m+\u001b[39m ngram] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m-\u001b[39m ngram \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)}\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "sentence = \"this is an example\"\n",
    "build_bigram_model(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
