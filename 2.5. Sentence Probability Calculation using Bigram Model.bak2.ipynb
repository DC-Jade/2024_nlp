{"cells":[{"cell_type":"markdown","metadata":{"id":"eS1h8tqidd9I"},"source":["**Tasks**\n","\n","In Task 2, we successfully implemented N-gram models, which laid the groundwork for understanding how words and their sequences can be analyzed to predict and interpret language patterns. Building upon that foundational knowledge, this assignment focuses specifically on the application of a bigram (2-gram) model to calculate the probability of a given sentence.\n","\n","1.   Implement Bigram Model Construction: Calculate and store the probability of each bigram based on the provided corpus.\n","2.   Calculate the Probability of a Given Sentence: Use the constructed bigram model to calculate the probability of a specific sentence.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_MwuhaddMap"},"outputs":[],"source":["def tokenize(text):\n","    # print(text)\n","    return text.lower().split()\n","\n","def padSequence(sequence_, ngram_, is_pad_left_ = True, is_pad_right_ = True,\n","                pad_left_token_ = \"<s>\", pad_right_token_ = \"</s>\"):\n","    assert isinstance(sequence_, list), \"sequence_ is not list\"\n","    if (is_pad_left_):\n","        i = 0\n","        while (i < ngram_ - 1):\n","            sequence_.insert(0, pad_left_token_)\n","            i += 1\n","    if (is_pad_right_):\n","        i = 0\n","        while (i < ngram_ - 1):\n","            sequence_.append(pad_right_token_)\n","            i += 1\n","    return sequence_\n","\n","def generate_ngrams(words, n):\n","    # TODO: Implement the logic to generate n-grams from the list of words\n","    words = padSequence(sequence_=words, ngram_=n)\n","    res = []\n","    res = [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n","    print(res)\n","    return res\n","\n","\n","def count_ngrams(ngrams):\n","    ngram_counts = {}\n","    for ngram in ngrams:\n","        if ngram in ngram_counts:\n","            ngram_counts[ngram] += 1\n","        else:\n","            ngram_counts[ngram] = 1\n","    # print(ngram_counts)\n","    return ngram_counts\n","\n","def calculate_ngram_probabilities(ngram_counts):\n","    total_ngrams = sum(ngram_counts.values())\n","    ngram_probabilities = {ngram: count / total_ngrams for ngram, count in ngram_counts.items()}\n","    return ngram_probabilities\n","\n","def is_near(lhs, rhs, delta=0.01):\n","    return(abs(lhs - rhs) < delta)\n","\n","def add_alpha_smoothing(unigram_counts, alpha, vocabulary_size):\n","    \"\"\"\n","    Apply Add-Î± smoothing to the unigram model.\n","    TODO: Implement the calculation of smoothed probabilities.\n","    \"\"\"\n","    smoothed_probabilities = {}\n","    # Your code here to calculate smoothed probabilities for each word\n","    # total_counts = sum(unigram_counts.values())\n","    n_token = len(unigram_counts.keys())\n","    # vocabulary_size_add_alpha = vocabulary_size + n_token * alpha\n","    vocabulary_size_add_alpha = sum(unigram_counts.values()) + n_token * alpha\n","    for unigram, count in unigram_counts.items():\n","        smoothed_prob = (count + alpha) / vocabulary_size_add_alpha\n","        smoothed_probabilities.update({unigram: smoothed_prob})\n","    # print(sum(smoothed_probabilities.values()))\n","    assert is_near(sum(smoothed_probabilities.values()),1), \"error: sum(prob) != 1\"\n","    return smoothed_probabilities\n","\n","def build_bigram_model(corpus):\n","    \"\"\"\n","    TODO: Build the bigram model\n","    - Count the occurrences of each bigram\n","    - Calculate the probability of each bigram based on counts\n","    \"\"\"\n","    bigram_counts = {}\n","    unigram_counts = {}\n","    # Your code here to populate bigram_counts and unigram_counts\n","    assert isinstance(corpus, str), \"error: input is not str\"\n","    words = tokenize(corpus)\n","    bigrams = generate_ngrams(words=words, n = 2)\n","    bigram_counts = count_ngrams(bigrams)\n","    \n","    # Calculate bigram probabilities\n","    bigram_probabilities = {}\n","    # Your code here to calculate probabilities from counts\n","    bigram_probabilities = add_alpha_smoothing(bigram_counts, alpha=1,\n","                                               vocabulary_size=0)\n","    # print(bigram_probabilities)\n","    return bigram_probabilities\n","\n","def build_unigram_model(corpus):\n","    \"\"\"\n","    - Count the occurrences of each unigram\n","    - Calculate the probability of each unigram based on counts\n","    \"\"\"\n","    unigram_counts = {}\n","    # Your code here to populate bigram_counts and unigram_counts\n","    assert isinstance(corpus, str), \"error: input is not str\"\n","    words = tokenize(corpus)\n","    unigrams = generate_ngrams(words=words, n = 1)\n","    unigram_counts = count_ngrams(unigrams)\n","    \n","    \n","    # Calculate bigram probabilities\n","    unigram_probabilities = {}\n","    # Your code here to calculate probabilities from counts\n","    unigram_probabilities = add_alpha_smoothing(unigram_counts, alpha=1,\n","                                                vocabulary_size=0)\n","    return unigram_probabilities\n","\n","# def conditional_bigram_probabilities(bigram_probabilities, unigram_probabilities):\n","def conditional_bigram_probabilities(bigram_probabilities):\n","    conditional_probabilities = {}\n","    bigrams = bigram_probabilities.items()\n","\n","    # calculate unigram probability\n","    unigram_list = [bigram.split() for bigram in bigram_probabilities.keys()]\n","    unigram_list = [bigram.split(sep = \" \") for bigram in bigram_probabilities.keys()]\n","    print(unigram_list)\n","    unigram_list = list(zip(*unigram_list))\n","    print(unigram_list)\n","    unigram_list = unigram_list[0]\n","    unigram_probability = {}\n","    unigram_counts = count_ngrams(unigram_list)\n","    print(unigram_counts)\n","    # unigram_probabilities = add_alpha_smoothing(unigram_counts, alpha=1, vocabulary_size=0)\n","    unigram_probabilities = calculate_ngram_probabilities(unigram_counts)\n","    \n","\n","    # calculate conditional probability   \n","    for bigram, prob in bigrams:\n","        bigram = bigram.split(sep = \" \")\n","        lhs = bigram[0]\n","        # word_rhs = bigram[1]\n","        print(bigram[0], prob)\n","        conditional_prob = prob / unigram_probabilities[lhs]    \n","        print({' '.join(bigram): conditional_prob})\n","        conditional_probabilities.update({' '.join(bigram): conditional_prob})\n","    print(conditional_probabilities)\n","    return(conditional_probabilities)\n","\n","# TODO here, add conditional probs\n","def calculate_sentence_probability(sentence, bigram_probabilities):\n","    \"\"\"\n","    TODO: Calculate the probability of a sentence using the bigram model\n","    - Split the sentence into words\n","    - Calculate the probability of each bigram in the sentence and multiply them to get the sentence probability\n","    \"\"\"\n","    probability = 1\n","    # Your code here to calculate the sentence probability\n","    # words = tokenize(sentence)\n","    unigram_probabilities = build_unigram_model(sentence)\n","    unigrams = list(unigram_probabilities.keys())\n","    conditional_probabilities = conditional_bigram_probabilities(bigram_probabilities)\n","\n","    total_words = len(unigrams)\n","    bigrams = bigram_probabilities.keys()\n","    for i in range(total_words - 1):\n","        # # bigram = ' '.join(list((unigrams[i], unigrams[i+1])))\n","        # if bigram in bigrams:\n","        # # bigram = (bigrams[i], bigrams[i+1])\n","        #     probability *= bigram_probabilities[bigram]\n","        #     print({bigram: probability})\n","        # else:\n","        #     probability *= 1 / len(bigrams)\n","        #     # pass\n","\n","        lhs = unigrams[i]\n","        rhs = unigrams[i+1]\n","        bigram = lhs + \" \" + rhs\n","        probability *= unigram_probabilities[lhs] * conditional_probabilities[bigram]\n","    # print(bigram_probabilities)\n","    # print(sum(bigram_probabilities.values()))\n","    return probability\n","\n","# Example corpus\n","corpus = \"this is an example sentence for the corpus it is just an example\"\n","# Build the bigram model\n","bigram_probabilities = build_bigram_model(corpus)\n","print(bigram_probabilities)\n","\n","# Calculate the probability of a given sentence\n","sentence = \"that is an example\"\n","# sentence = \"this is an example sentence for the corpus it is just an\"\n","probability = calculate_sentence_probability(sentence, bigram_probabilities)\n","print(f\"Sentence Probability: {probability}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1bH1vqaupc1r7uM_0Hd9FaBBM499ccMao","timestamp":1710553808038}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
