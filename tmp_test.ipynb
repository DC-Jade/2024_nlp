{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (\"This movie was an excellent adventure with fantastic visuals\", \"Positive\"),\n",
    "    (\"I loved the storyline and the characters were very compelling\", \"Positive\"),\n",
    "    (\"One of the best movies I have seen in recent years\", \"Positive\"),\n",
    "    (\"A thrilling movie that kept me on the edge of my seat from start to finish\", \"Positive\"),\n",
    "    (\"The cinematography is stunning and the plot is intriguing\", \"Positive\"),\n",
    "    (\"This movie was a waste of time and the plot was very predictable\", \"Negative\"),\n",
    "    (\"I was disappointed with the lead actor's performance and the movie was dull\", \"Negative\"),\n",
    "    (\"Not worth the ticket price - I found it to be quite boring\", \"Negative\"),\n",
    "    (\"The movie failed to deliver on its promise of a thrilling experience\", \"Negative\"),\n",
    "    (\"It was an underwhelming movie that left much to be desired\", \"Negative\"),\n",
    "    (\"The script felt rushed and the ending was unsatisfying\", \"Negative\"),\n",
    "    (\"An absolute masterpiece, with brilliant direction and performances\", \"Positive\"),\n",
    "    (\"The film’s pacing was perfect, and the suspense was maintained throughout\", \"Positive\"),\n",
    "    (\"A highly recommended movie that appeals to both critics and the general audience\", \"Positive\"),\n",
    "    (\"A unique and beautiful take on an otherwise familiar story\", \"Positive\"),\n",
    "    (\"The dialogue was uninspired, and the pacing was slow\", \"Negative\"),\n",
    "    (\"Poor character development and a predictable plot\", \"Negative\"),\n",
    "    (\"I found the film to be overrated and lacking in originality\", \"Negative\"),\n",
    "    (\"The special effects were mediocre at best, failing to impress\", \"Negative\"),\n",
    "    (\"A forgettable film that doesn't leave a lasting impression\", \"Negative\")\n",
    "]\n",
    "\n",
    "# Separate the documents and their labels\n",
    "docs, labels = zip(*documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?zip\n",
    "print(documents)\n",
    "print(*documents)\n",
    "# print(docs)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = range(4)\n",
    "# words = words[0:3]\n",
    "for word in words:\n",
    "    print(word)\n",
    "print(words[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Example token list\n",
    "tokens = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "\n",
    "# Define the value of n for n-grams\n",
    "n = 2  # Change this value to generate different n-grams (e.g., 3 for trigrams)\n",
    "\n",
    "# Generate n-grams using nltk's ngrams function\n",
    "ngrams_list = ngrams(tokens, 3)\n",
    "print(ngrams_list)\n",
    "\n",
    "# Print the generated n-grams\n",
    "\n",
    "for gram in ngrams_list:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Probability: 0\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    # print(text)\n",
    "    return text.lower().split()\n",
    "\n",
    "def padSequence(sequence_, ngram_, is_pad_left_ = True, is_pad_right_ = True,\n",
    "                pad_left_token_ = \"<s>\", pad_right_token_ = \"</s>\"):\n",
    "    assert isinstance(sequence_, list), \"sequence_ is not list\"\n",
    "    if (is_pad_left_):\n",
    "        i = 0\n",
    "        while (i < ngram_ - 1):\n",
    "            sequence_.insert(0, pad_left_token_)\n",
    "            i += 1\n",
    "    if (is_pad_right_):\n",
    "        i = 0\n",
    "        while (i < ngram_ - 1):\n",
    "            sequence_.append(pad_right_token_)\n",
    "            i += 1\n",
    "    return sequence_\n",
    "\n",
    "def generate_ngrams(words, n):\n",
    "    # TODO: Implement the logic to generate n-grams from the list of words\n",
    "    words = padSequence(sequence_=words, ngram_=n)\n",
    "    res = [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "    return res\n",
    "\n",
    "def count_ngrams(ngrams):\n",
    "    ngram_counts = {}\n",
    "    for ngram in ngrams:\n",
    "        if ngram in ngram_counts:\n",
    "            ngram_counts[ngram] += 1\n",
    "        else:\n",
    "            ngram_counts[ngram] = 1\n",
    "    return ngram_counts\n",
    "\n",
    "def calculate_ngram_probabilities(ngram_counts):\n",
    "    total_ngrams = sum(ngram_counts.values())\n",
    "    ngram_probabilities = {ngram: count / total_ngrams for ngram, count in ngram_counts.items()}\n",
    "    return ngram_probabilities\n",
    "\n",
    "def add_alpha_smoothing(unigram_counts, alpha, vocabulary_size):\n",
    "    \"\"\"\n",
    "    Apply Add-α smoothing to the unigram model.\n",
    "    TODO: Implement the calculation of smoothed probabilities.\n",
    "    \"\"\"\n",
    "    smoothed_probabilities = {}\n",
    "    # Your code here to calculate smoothed probabilities for each word\n",
    "    # total_counts = sum(unigram_counts.values())\n",
    "    n_token = len(unigram_counts.keys())\n",
    "    # vocabulary_size_add_alpha = vocabulary_size + n_token * alpha\n",
    "    vocabulary_size_add_alpha = sum(unigram_counts.values()) + n_token * alpha\n",
    "    for unigram, count in unigram_counts.items():\n",
    "        smoothed_prob = (count + alpha) / vocabulary_size_add_alpha\n",
    "        smoothed_probabilities.update({unigram: smoothed_prob})\n",
    "    # print(sum(smoothed_probabilities.values()))\n",
    "    assert is_near(sum(smoothed_probabilities.values()),1), \"error: sum(prob) != 1\"\n",
    "    return smoothed_probabilities\n",
    "\n",
    "def build_bigram_model(corpus):\n",
    "    \"\"\"\n",
    "    TODO: Build the bigram model\n",
    "    - Count the occurrences of each bigram\n",
    "    - Calculate the probability of each bigram based on counts\n",
    "    \"\"\"\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "    # Your code here to populate bigram_counts and unigram_counts\n",
    "    assert isinstance(corpus, str), \"error: input is not str\"\n",
    "    words = tokenize(corpus)\n",
    "    bigrams = generate_ngrams(words=words, n = 2)\n",
    "    bigram_counts = count_ngrams(bigrams)\n",
    "    \n",
    "    # Calculate bigram probabilities\n",
    "    bigram_probabilities = {}\n",
    "    # Your code here to calculate probabilities from counts\n",
    "    bigram_probabilities = add_alpha_smoothing(bigram_counts, alpha=1,\n",
    "                                               vocabulary_size=0)\n",
    "    # print(bigram_probabilities)\n",
    "    return bigram_probabilities\n",
    "\n",
    "def build_unigram_model(corpus):\n",
    "    \"\"\"\n",
    "    - Count the occurrences of each unigram\n",
    "    - Calculate the probability of each unigram based on counts\n",
    "    \"\"\"\n",
    "    unigram_counts = {}\n",
    "    # Your code here to populate bigram_counts and unigram_counts\n",
    "    assert isinstance(corpus, str), \"error: input is not str\"\n",
    "    words = tokenize(corpus)\n",
    "    unigrams = generate_ngrams(words=words, n = 1)\n",
    "    unigram_counts = count_ngrams(unigrams)\n",
    "    \n",
    "    # Calculate bigram probabilities\n",
    "    unigram_probabilities = {}\n",
    "    # Your code here to calculate probabilities from counts\n",
    "    unigram_probabilities = add_alpha_smoothing(unigram_counts, alpha=1,\n",
    "                                                vocabulary_size=len(unigram_counts))\n",
    "    # assert sum(bigram_probabilities.values()) == 1, \"error: sum(prob) != 1\"\n",
    "    return unigram_probabilities\n",
    "\n",
    "# def conditional_bigram_probabilities(bigram_probabilities, unigram_probabilities):\n",
    "#     conditional_probabilities = {}\n",
    "#     bigrams = bigram_probabilities.items()\n",
    "#     for bigram, prob in bigrams:\n",
    "#         bigram = bigram.split()\n",
    "#         word_lhs = bigram[0]\n",
    "#         # word_rhs = bigram[1]\n",
    "#         conditional_prob = prob / unigram_probabilities[word_lhs]\n",
    "#         conditional_probabilities[bigram] = conditional_prob\n",
    "#     return(conditional_bigram_probabilities)\n",
    "\n",
    "\n",
    "def calculate_sentence_probability(sentence, bigram_probabilities):\n",
    "    \"\"\"\n",
    "    TODO: Calculate the probability of a sentence using the bigram model\n",
    "    - Split the sentence into words\n",
    "    - Calculate the probability of each bigram in the sentence and multiply them to get the sentence probability\n",
    "    \"\"\"\n",
    "    probability = 1\n",
    "    # Your code here to calculate the sentence probability\n",
    "    # words = tokenize(sentence)\n",
    "    unigram_probabilities = build_unigram_model(sentence)\n",
    "    unigrams = list(unigram_probabilities.keys())\n",
    "    # conditional_probabilities = conditional_bigram_probabilities(bigram_probabilities, unigram_probabilities)\n",
    "\n",
    "    total_words = len(unigrams)\n",
    "    bigrams = bigram_probabilities.keys()\n",
    "    for i in range(total_words - 1):\n",
    "        bigram = ' '.join(list((unigrams[i], unigrams[i+1])))\n",
    "        if bigram in bigrams:\n",
    "        # bigram = (bigrams[i], bigrams[i+1])\n",
    "            probability *= bigram_probabilities[bigram]\n",
    "        else:\n",
    "            probability = 0\n",
    "\n",
    "    return probability\n",
    "\n",
    "# Example corpus\n",
    "corpus = \"this is an example sentence for the corpus it is just an example\"\n",
    "# Build the bigram model\n",
    "bigram_probabilities = build_bigram_model(corpus)\n",
    "\n",
    "# Calculate the probability of a given sentence\n",
    "sentence = \"this is an example sentence for the corpus it is just an example\" #\"this is an example\"\n",
    "probability = calculate_sentence_probability(sentence, bigram_probabilities)\n",
    "print(f\"Sentence Probability: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_train = \"this is an example sentence for the corpus it is just an example\"\n",
    "sentence_train = \"this is an example sentence for the corpus it is just an example sentence_train this is an example sentence for the corpus it is just an example\"\n",
    "sentence_test = \"this is an example sentence for the corpus it is just an example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0.07894736842105263,\n",
       " 'is': 0.13157894736842105,\n",
       " 'an': 0.13157894736842105,\n",
       " 'example': 0.13157894736842105,\n",
       " 'sentence': 0.07894736842105263,\n",
       " 'for': 0.07894736842105263,\n",
       " 'the': 0.07894736842105263,\n",
       " 'corpus': 0.07894736842105263,\n",
       " 'it': 0.07894736842105263,\n",
       " 'just': 0.07894736842105263,\n",
       " 'sentence_train': 0.05263157894736842}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_unigram_model(sentence_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(build_bigram_model(sentence_train).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = \"hello\"\n",
    "rhs = \"world\"\n",
    "\n",
    "bigram = lhs + \" \"  + rhs\n",
    "bigrams = [bigram, bigram, bigram]\n",
    "bigram_list = [str.split() for str in bigrams]\n",
    "# print(bigram_list)\n",
    "bigram_list = list(zip(*bigram_list))\n",
    "for i in bigram_list:\n",
    "    # print(i) \n",
    "    pass\n",
    "unigram_list = bigram_list[0]\n",
    "print(unigram_list)\n",
    "# print(bigram_list)\n",
    "# unigram = bigram_list[0]\n",
    "# print(unigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?str.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_near(lhs, rhs, delta=0.01):\n",
    "    return(abs(lhs - rhs) < delta)\n",
    "\n",
    "def add_alpha_smoothing(unigram_counts, alpha, vocabulary_size):\n",
    "    \"\"\"\n",
    "    Apply Add-α smoothing to the unigram model.\n",
    "    TODO: Implement the calculation of smoothed probabilities.\n",
    "    \"\"\"\n",
    "    smoothed_probabilities = {}\n",
    "    # Your code here to calculate smoothed probabilities for each word\n",
    "    # total_counts = sum(unigram_counts.values())\n",
    "    n_token = len(unigram_counts.keys())\n",
    "    # vocabulary_size_add_alpha = vocabulary_size + n_token * alpha\n",
    "    vocabulary_size_add_alpha = sum(unigram_counts.values()) + n_token * alpha\n",
    "    for unigram, count in unigram_counts.items():\n",
    "        smoothed_prob = (count + alpha) / vocabulary_size_add_alpha\n",
    "        smoothed_probabilities.update({unigram: smoothed_prob})\n",
    "    # print(sum(smoothed_probabilities.values()))\n",
    "    assert is_near(sum(smoothed_probabilities.values()),1), \"error: sum(prob) != 1\"\n",
    "    return smoothed_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = build_bigram_model(sentence).items()\n",
    "print(bigrams)\n",
    "bigrams = bigram_probabilities.items()\n",
    "# calculate unigram probability\n",
    "unigram_list = [bigram.split(sep = \" \") for bigram in bigram_probabilities.keys()]\n",
    "print(unigram_list)\n",
    "unigram_list = list(zip(*unigram_list))\n",
    "print(unigram_list)\n",
    "unigram_list = unigram_list[0]\n",
    "print(list(unigram_list[0]))\n",
    "unigram_counts = count_ngrams(unigram_list)\n",
    "\n",
    "print(unigram_counts)\n",
    "unigram_probability = add_alpha_smoothing(unigram_counts, alpha=1,   vocabulary_size=0)\n",
    "print(sum(unigram_probability.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'sat', 'on', 'the', 'mat.', 'the', 'dog', 'barked', 'at', 'the', 'cat.']\n",
      "[('the', 'cat'), ('cat', 'sat'), ('sat', 'on'), ('on', 'the'), ('the', 'mat.'), ('mat.', 'the'), ('the', 'dog'), ('dog', 'barked'), ('barked', 'at'), ('at', 'the'), ('the', 'cat.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import bigrams, ConditionalFreqDist\n",
    "text = \"The cat sat on the mat. The dog barked at the cat.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# Generate bigrams from the tokens\n",
    "bi_tokens = list(bigrams(tokens))\n",
    "print(bi_tokens)\n",
    "\n",
    "# Create a ConditionalFreqDist to count occurrences of bigrams and unigrams\n",
    "# cfd = ConditionalFreqDist((word1, word2) for word1, word2 in bi_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
